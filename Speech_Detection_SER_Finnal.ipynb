{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqQTpvdyhW_t",
        "outputId": "36274c9d-7f0d-41a2-e941-ecc6aa66dc06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: pyannote.audio in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.26.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.4.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.8.0)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (5.1.0)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.0.1)\n",
            "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.7.0)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (13.9.4)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.0.2)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.6.2.2)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.11.1)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (1.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio) (0.11.9)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.15.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.8.0)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (4.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio) (4.25.5)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.5)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain) (0.18.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.8.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.36)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.10/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.18.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.1.1)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain flask pyngrok torchaudio librosa transformers pyannote.audio torch\n",
        "!ngrok authtoken <ngtoken>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mZryTopiB-q",
        "outputId": "f5677ca6-6f6a-407a-8301-d9716227e19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authenticated as: {'type': 'user', 'id': '67351a46640167c53e35c716', 'name': 'Sankalp-Dwivedi', 'fullname': 'Sankalp Dwivedi', 'isPro': False, 'avatarUrl': '/avatars/ac5b974b164ea7f52b4f43c5337c5917.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'access-token', 'role': 'fineGrained', 'createdAt': '2024-11-13T21:37:59.973Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['inference.serverless.write', 'discussion.write', 'post.write'], 'scoped': [{'entity': {'_id': '621ffdc136468d709f17f135', 'type': 'model'}, 'permissions': ['repo.content.read']}, {'entity': {'_id': '67351a46640167c53e35c716', 'type': 'user', 'name': 'Sankalp-Dwivedi'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.endpoints.infer.write', 'inference.endpoints.write', 'user.webhooks.read', 'user.webhooks.write', 'collection.read', 'collection.write', 'discussion.write']}]}}}}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Set your token explicitly\n",
        "HUGGINGFACE_TOKEN = \"<HGToken>\"\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = HUGGINGFACE_TOKEN\n",
        "HfFolder.save_token(HUGGINGFACE_TOKEN)\n",
        "\n",
        "# Test if authentication works by trying to access the API\n",
        "api = HfApi()\n",
        "user_info = api.whoami()\n",
        "print(\"Authenticated as:\", user_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mb-IHcdiIjJ"
      },
      "outputs": [],
      "source": [
        "home_template=\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Emotion & Gender Detection</title>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            background-color: #121212; /* Dark background */\n",
        "            color: #ffffff; /* Light text */\n",
        "            font-family: 'Arial', sans-serif;\n",
        "        }\n",
        "\n",
        "        h1 {\n",
        "            font-weight: bold;\n",
        "            font-size: 3rem;\n",
        "            background: linear-gradient(90deg, #ff6f61, #58a6ff);\n",
        "            -webkit-background-clip: text;\n",
        "            -webkit-text-fill-color: transparent;\n",
        "        }\n",
        "\n",
        "        p.lead {\n",
        "            font-size: 1.25rem;\n",
        "            color: #d1d1d1;\n",
        "        }\n",
        "\n",
        "        .btn-primary {\n",
        "            background-color: #ff6f61;\n",
        "            border: none;\n",
        "            box-shadow: 0 4px 15px rgba(255, 111, 97, 0.6);\n",
        "            font-size: 1.2rem;\n",
        "            font-weight: bold;\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .btn-primary:hover {\n",
        "            background-color: #58a6ff;\n",
        "            box-shadow: 0 6px 20px rgba(88, 166, 255, 0.7);\n",
        "        }\n",
        "\n",
        "        .btn-analyze {\n",
        "            background: linear-gradient(90deg, #1e90ff, #00c853);\n",
        "            border: none;\n",
        "            border-radius: 50px;\n",
        "            padding: 12px 20px;\n",
        "            font-size: 1.1rem;\n",
        "            font-weight: bold;\n",
        "            color: #ffffff;\n",
        "            box-shadow: 0 4px 15px rgba(30, 144, 255, 0.6);\n",
        "            transition: all 0.3s ease-in-out;\n",
        "        }\n",
        "\n",
        "        .btn-analyze:hover {\n",
        "            background: linear-gradient(90deg, #00c853, #1e90ff);\n",
        "            box-shadow: 0 6px 25px rgba(0, 200, 83, 0.7);\n",
        "        }\n",
        "\n",
        "        .container {\n",
        "            margin-top: 10%;\n",
        "        }\n",
        "\n",
        "        .text-center {\n",
        "            border-radius: 12px;\n",
        "            padding: 40px;\n",
        "            background: linear-gradient(145deg, #1c1c1c, #292929);\n",
        "            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.5);\n",
        "        }\n",
        "\n",
        "        a.btn {\n",
        "            text-decoration: none;\n",
        "        }\n",
        "\n",
        "        @media (max-width: 768px) {\n",
        "            h1 {\n",
        "                font-size: 2.5rem;\n",
        "            }\n",
        "\n",
        "            p.lead {\n",
        "                font-size: 1rem;\n",
        "            }\n",
        "\n",
        "            .btn-primary {\n",
        "                font-size: 1rem;\n",
        "            }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <div class=\"text-center\">\n",
        "            <h1>Emotion & Gender Detection</h1>\n",
        "            <p class=\"lead\">\n",
        "                Welcome to our AI-powered audio emotion and gender detection tool. This project uses cutting-edge\n",
        "                machine learning models to analyze audio files, identify the speaker's emotion, and estimate their gender.\n",
        "            </p>\n",
        "            <p>\n",
        "                Built with the Hugging Face Wav2Vec2 model, this tool represents state-of-the-art advancements in\n",
        "                speech recognition and natural language processing. Experience AI's potential in understanding human speech.\n",
        "            </p>\n",
        "            <a href=\"/upload\" class=\"btn btn-analyze btn-lg mt-4\">Upload Audio File</a>\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri6JgY7viRAO"
      },
      "outputs": [],
      "source": [
        "#latest 2\n",
        "upload_template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Upload Audio File</title>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            background-color: #121212; /* Dark theme */\n",
        "            color: #ffffff; /* Light text */\n",
        "            font-family: 'Arial', sans-serif;\n",
        "        }\n",
        "\n",
        "        h1 {\n",
        "            font-weight: bold;\n",
        "            font-size: 2.5rem;\n",
        "            background: linear-gradient(90deg, #58a6ff, #ff6f61);\n",
        "            -webkit-background-clip: text;\n",
        "            -webkit-text-fill-color: transparent;\n",
        "            text-align: center;\n",
        "        }\n",
        "\n",
        "        p {\n",
        "            color: #00000;\n",
        "            text-align: center;\n",
        "        }\n",
        "\n",
        "        .btn-analyze {\n",
        "            background: linear-gradient(90deg, #1e90ff, #00c853);\n",
        "            border: none;\n",
        "            border-radius: 50px;\n",
        "            padding: 12px 20px;\n",
        "            font-size: 1.1rem;\n",
        "            font-weight: bold;\n",
        "            color: #ffffff;\n",
        "            box-shadow: 0 4px 15px rgba(30, 144, 255, 0.6);\n",
        "            transition: all 0.3s ease-in-out;\n",
        "        }\n",
        "\n",
        "        .btn-analyze:hover {\n",
        "            background: linear-gradient(90deg, #00c853, #1e90ff);\n",
        "            box-shadow: 0 6px 25px rgba(0, 200, 83, 0.7);\n",
        "        }\n",
        "\n",
        "        .form-label {\n",
        "            color: #bdbdbd;\n",
        "        }\n",
        "\n",
        "        .container {\n",
        "            margin-top: 8%;\n",
        "            padding: 40px;\n",
        "            background: linear-gradient(145deg, #1c1c1c, #292929);\n",
        "            border-radius: 12px;\n",
        "            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.5);\n",
        "        }\n",
        "\n",
        "        #result {\n",
        "            font-size: 1.2rem;\n",
        "        }\n",
        "\n",
        "        #result .alert {\n",
        "            animation: fadeIn 0.8s ease-in-out;\n",
        "        }\n",
        "\n",
        "        @keyframes fadeIn {\n",
        "            from {\n",
        "                opacity: 0;\n",
        "                transform: scale(0.9);\n",
        "            }\n",
        "            to {\n",
        "                opacity: 1;\n",
        "                transform: scale(1);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        .loader {\n",
        "            display: inline-block;\n",
        "            width: 24px;\n",
        "            height: 24px;\n",
        "            border: 3px solid rgba(255, 255, 255, 0.3);\n",
        "            border-radius: 50%;\n",
        "            border-top-color: #ffffff;\n",
        "            animation: spin 1s linear infinite;\n",
        "        }\n",
        "\n",
        "        @keyframes spin {\n",
        "            to {\n",
        "                transform: rotate(360deg);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        @media (max-width: 768px) {\n",
        "            h1 {\n",
        "                font-size: 2rem;\n",
        "            }\n",
        "\n",
        "            .btn-analyze {\n",
        "                font-size: 1rem;\n",
        "            }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>Upload and Analyze Audio</h1>\n",
        "        <p>\n",
        "            Upload an audio file to detect the speaker's emotion and gender with advanced AI models.\n",
        "        </p>\n",
        "        <div class=\"row justify-content-center\">\n",
        "            <div class=\"col-md-6\">\n",
        "                <form id=\"uploadForm\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "                    <div class=\"mb-3\">\n",
        "                        <label for=\"file\" class=\"form-label\">Select Audio File</label>\n",
        "                        <input type=\"file\" class=\"form-control\" id=\"file\" name=\"file\" accept=\"audio/*\" required>\n",
        "                    </div>\n",
        "                    <div class=\"mb-3\" id=\"audioPlayerContainer\" style=\"display: none;\">\n",
        "                        <label class=\"form-label\">Preview Audio:</label>\n",
        "                        <audio id=\"audioPlayer\" controls></audio>\n",
        "                    </div>\n",
        "                    <button type=\"submit\" class=\"btn btn-analyze w-100\">Analyze</button>\n",
        "                </form>\n",
        "                <div id=\"result\" class=\"mt-4\"></div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "    <script>\n",
        "        const fileInput = document.getElementById('file');\n",
        "        const audioPlayerContainer = document.getElementById('audioPlayerContainer');\n",
        "        const audioPlayer = document.getElementById('audioPlayer');\n",
        "\n",
        "        // Update audio player preview\n",
        "        fileInput.addEventListener('change', function () {\n",
        "            const file = fileInput.files[0];\n",
        "            if (file) {\n",
        "                const objectUrl = URL.createObjectURL(file);\n",
        "                audioPlayer.src = objectUrl;\n",
        "                audioPlayerContainer.style.display = 'block';\n",
        "            } else {\n",
        "                audioPlayer.src = '';\n",
        "                audioPlayerContainer.style.display = 'none';\n",
        "            }\n",
        "        });\n",
        "\n",
        "        // Submit form and show loader\n",
        "        document.getElementById('uploadForm').onsubmit = async function (e) {\n",
        "            e.preventDefault();\n",
        "            const formData = new FormData(this);\n",
        "            const resultDiv = document.getElementById('result');\n",
        "            resultDiv.innerHTML = '<div class=\"text-center\"><span class=\"loader\"></span> Processing...</div>';\n",
        "            try {\n",
        "                const response = await fetch('/analyze', { method: 'POST', body: formData });\n",
        "                const data = await response.json();\n",
        "                if (data.error) {\n",
        "                    resultDiv.innerHTML = `<div class=\"text-center text-danger\">${data.error}</div>`;\n",
        "                } else {\n",
        "                    // Adding emojis based on emotion\n",
        "                    const emotionEmojiMap = {\n",
        "                        happy: 'üòä',\n",
        "                        sad: 'üò¢',\n",
        "                        angry: 'üò°',\n",
        "                        neutral: 'üòê',\n",
        "                        surprised: 'üò≤'\n",
        "                    };\n",
        "                    const genderEmoji = data.gender.toLowerCase() === 'male' ? 'üë®' : 'üë©';\n",
        "                    if (data.gender.toLowerCase() === 'unknown'){\n",
        "                        genderEmoji = '‚ùî';\n",
        "                    }\n",
        "                    resultDiv.innerHTML = `\n",
        "                        <div class=\"alert alert-success\">\n",
        "                            <p><strong>Emotion:</strong> ${data.emotion} ${emotionEmojiMap[data.emotion.toLowerCase()] || 'üé≠'}</p>\n",
        "                            <p><strong>Gender:</strong> ${data.gender} ${genderEmoji}</p>\n",
        "                            <p><strong>Text:</strong> ${data.text}</p>\n",
        "                            <p><strong>Sentiment:</strong> ${data.sentiment}</p>\n",
        "                        </div>`;\n",
        "                }\n",
        "            } catch (error) {\n",
        "                resultDiv.innerHTML = `<div class=\"text-center text-danger\">Error: ${error.message}</div>`;\n",
        "            }\n",
        "        };\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzEis8kZia7n",
        "outputId": "5ed4b6c5-b095-4f3c-b56a-0c8b9b6f200f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on public URL: NgrokTunnel: \"https://d021-34-16-227-53.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Dec/2024 19:40:35] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Dec/2024 19:40:36] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Dec/2024 19:40:37] \"GET /upload HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emotion:\n",
            "Logits: tensor([[ 0.1049,  0.0821, -0.0334, -0.0457,  0.1332,  0.0478, -0.0252]])\n",
            "Probabilities: tensor([[0.1524, 0.1490, 0.1328, 0.1311, 0.1568, 0.1440, 0.1339]])\n",
            "Predicted class: 4\n",
            "^^^^^^^\n",
            "[{'score': 0.998764157295227, 'label': 'male'}, {'score': 0.0012359129032120109, 'label': 'female'}]\n",
            "male\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Dec/2024 19:40:57] \"POST /analyze HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcribed Text: KIDS ARE TALKING BY THE DOOR\n",
            "Sentiment Analysis: [{'label': 'POSITIVE', 'score': 0.980668306350708}]\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import os\n",
        "from flask import Flask, request, jsonify, render_template, render_template_string\n",
        "from pyngrok import ngrok\n",
        "from transformers import pipeline\n",
        "import torchaudio\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.audio.pipelines import SpeakerDiarization\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, pipeline\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "\n",
        "emotion_model_name = \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
        "emotion_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(emotion_model_name)\n",
        "emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(emotion_model_name)\n",
        "\n",
        "\n",
        "gender_model_name = \"alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech\"\n",
        "gender_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(gender_model_name)\n",
        "gender_model = Wav2Vec2ForSequenceClassification.from_pretrained(gender_model_name)\n",
        "\n",
        "def analyze_audio(file_path):\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        emotion = detect_emotion(waveform.clone(), sample_rate)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # gender = detect_gender(waveform.clone(), 16000)\n",
        "        gender = detect_gender(file_path, 16000)\n",
        "\n",
        "        text, sentiment = sentiment_finding(file_path)\n",
        "\n",
        "        return emotion, gender , text, sentiment\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analyze_audio: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def detect_emotion(waveform, sample_rate):\n",
        "    try:\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        if waveform.dim() > 1:\n",
        "            waveform = waveform.mean(dim=0).unsqueeze(0)\n",
        "\n",
        "        waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "        input_values = emotion_feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = emotion_model(input_values).logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probabilities).item()\n",
        "\n",
        "        emotion_labels = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "        print(\"Emotion:\")\n",
        "        print(\"Logits:\", logits)\n",
        "        print(\"Probabilities:\", probabilities)\n",
        "        print(\"Predicted class:\", predicted_class)\n",
        "        print(\"^^^^^^^\")\n",
        "\n",
        "        return emotion_labels[predicted_class]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in detect_emotion: {e}\")\n",
        "        return \"Unknown\"\n",
        "\n",
        "def preprocess_audio(file_path, sampling_rate=16000):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    if sample_rate != sampling_rate:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sampling_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    if waveform.size(0) > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    waveform = waveform / waveform.abs().max()\n",
        "\n",
        "    return waveform.squeeze(0).numpy()\n",
        "\n",
        "def detect_gender(file_path, sample_rate):\n",
        "    try:\n",
        "        # if waveform.dim() > 1:\n",
        "        #     waveform = waveform.mean(dim=0).unsqueeze(0)\n",
        "\n",
        "        # input_values = gender_feature_extractor(waveform.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=sample_rate).input_values\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     logits = gender_model(input_values).logits\n",
        "        #     probabilities = torch.softmax(logits, dim=1)\n",
        "        #     predicted_class = torch.argmax(logits).item()\n",
        "\n",
        "        # print(\"Gender:\")\n",
        "        # print(\"Logits:\", logits)\n",
        "        # print(\"Probabilities:\", probabilities)\n",
        "        # print(predicted_class)\n",
        "        # print(\"^^^^^^^\")\n",
        "\n",
        "        # return \"Female\" if predicted_class == 1 else \"Male\"\n",
        "\n",
        "        pipe = pipeline(\"audio-classification\", model=\"alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech\")\n",
        "        audio_array = preprocess_audio(file_path)\n",
        "        results = pipe(audio_array)\n",
        "        print(results)\n",
        "        max_label = max(results, key=lambda x: x['score'])['label']\n",
        "        print(max_label)\n",
        "\n",
        "        return max_label\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in detect_gender: {e}\")\n",
        "        return \"Unknown\"\n",
        "\n",
        "\n",
        "def sentiment_finding(file_path):\n",
        "\n",
        "  asr_pipeline = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-large-960h\")\n",
        "\n",
        "  sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "  audio_array = preprocess_audio(file_path)\n",
        "\n",
        "  transcribed_text = asr_pipeline(audio_array)['text']\n",
        "\n",
        "  sentiment_result = sentiment_pipeline(transcribed_text)\n",
        "\n",
        "  print(f\"Transcribed Text: {transcribed_text}\")\n",
        "  print(f\"Sentiment Analysis: {sentiment_result}\")\n",
        "  return transcribed_text, max(sentiment_result, key=lambda x: x['score'])['label']\n",
        "\n",
        "# Homepage route\n",
        "@app.route('/')\n",
        "def home():\n",
        "    # return render_template(\"home.html\")\n",
        "    return render_template_string(home_template)\n",
        "\n",
        "# Audio analysis page route\n",
        "@app.route('/upload')\n",
        "def upload():\n",
        "    # return render_template(\"upload.html\")\n",
        "    return render_template_string(upload_template)\n",
        "\n",
        "# Analysis endpoint\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def analyze():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No file uploaded\"}), 400\n",
        "\n",
        "    audio_file = request.files['file']\n",
        "    file_path = \"/tmp/uploaded_audio.wav\"\n",
        "    audio_file.save(file_path)\n",
        "\n",
        "    # Run audio analysis\n",
        "    emotion, gender, text, sentiment = analyze_audio(file_path)\n",
        "    if emotion and gender:\n",
        "        return jsonify({\"emotion\": emotion, \"gender\": gender, \"text\": text, \"sentiment\": sentiment})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Could not process audio file\"}), 500\n",
        "\n",
        "# Run Flask app with ngrok\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"Running on public URL:\", public_url)\n",
        "    app.run(port=5000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgsIYclq94Rl",
        "outputId": "f92553cc-4ff9-48b0-ca1d-e7e0ce2d8439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'score': 0.9986415505409241, 'label': 'female'}, {'score': 0.0013584708794951439, 'label': 'male'}]\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, AutoProcessor, AutoModelForAudioClassification\n",
        "\n",
        "def preprocess_audio(file_path, sampling_rate=16000):\n",
        "    # Load audio\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # Resample if needed\n",
        "    if sample_rate != sampling_rate:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sampling_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono\n",
        "    if waveform.size(0) > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Normalize\n",
        "    waveform = waveform / waveform.abs().max()\n",
        "\n",
        "    return waveform.squeeze(0).numpy()\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"audio-classification\", model=\"alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech\")\n",
        "\n",
        "\n",
        "audio_array = preprocess_audio(\"female.wav\")\n",
        "results = pipe(audio_array)\n",
        "max_label = max(results, key=lambda x: x['score'])['label']\n",
        "print(max_label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
